# Motor City Math â€” Marcus Queue

> Items needing Marcus's input or decisions. Agents write here, Marcus reads and responds.
> Severity: ðŸ”´ BLOCKER (can't proceed) Â· ðŸŸ¡ DECISION (need a choice) Â· ðŸŸ¢ FYI (informational)

---

## Status Summary

**Last updated:** 2026-02-17
**Project:** 12 HTML practice tests, ~200 questions, ~14,700 LOC
**Agents:** 6 registered, 3 active (Design, Research, Platform), 3 awaiting onboarding
**Critical:** 24 math/grading bugs filed by Agent R â€” see below

---

## Open Items

### ðŸŸ¢ FYI â€” Agent R: Question bank and SRS design complete
**Filed:** 2026-02-17 by Research Agent

Marcus, the foundation is built:

- **`data/questions.json`** â€” 142 questions extracted from all 12 HTML files. Every question has: ID, unit, standard, difficulty, type, hint (â‰¤12 words), and step-by-step solution. Known math errors are corrected in JSON.
- **`data/standards.json`** â€” Full SAAS Algebra II curriculum registry (6 units, 31 standards with descriptions, difficulty rubric).
- **`docs/spaced-repetition-design.md`** â€” Modified Leitner spaced repetition system. 6-box intervals (session â†’ 1d â†’ 3d â†’ 7d â†’ 14d), 15-question session cap for ADHD, localStorage data model, queue algorithm. Ready for Agent A to implement.

**What this enables:** Agent A can now build the question renderer (A-12), randomized practice mode (A-13), and test builder (A-14) using the structured question bank. The SRS algorithm is ready to power adaptive review sessions.

**Remaining R-agent work:** R-11 (parse Winter Tri worksheets), R-12 (generate 50 new questions per unit).

---

**Filed:** 2026-02-17 by Research Agent

Marcus, the math audit is complete. **8 of 12 files are clean.** But 4 files have significant errors:

**`index.html` â€” Grading is 100% broken.** The auto-grading JavaScript was copy-pasted from `index_calc.html` and doesn't match index.html's questions at all. Every auto-graded answer would be wrong, and the function crashes on Q11/Q14 (elements don't exist). The answer key MODAL is mostly correct â€” Kai likely used that, not auto-grading. **This is why he still got an A.**

**`final_exam_251123.html` â€” 6 wrong headline answers.** The work steps are correct but the bolded answer numbers are wrong (Q1: -234â†’210, Q5: -3â†’-9, Q6: sign error, Q7: 12/13â†’6/5, Q16b: off by one period, Q17b: computation error). If Kai reads the work, he gets the right answer. If he just reads the bold number, he learns the wrong thing.

**`index_calc.html` â€” 2 computation errors.** Q6d (62â†’66Â°F) and Q16b ($14,636.56â†’$14,616.18).

**Decision needed:** Should Agent A fix these now (before any other work), or should we prioritize the shared CSS/JS extraction first? My recommendation: fix the answer keys first â€” they're surgical changes (just number replacements) and directly affect Kai's learning.

Full bug list with correct answers is in `.agent-status.md` â†’ Bugs section.

---

## Resolved Items

<details>
<summary>Click to expand resolved items</summary>

_(none yet)_

</details>
